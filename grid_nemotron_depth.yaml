# Nemotron Depth Experiment - Dec 22, 2025
# Testing: Does depth (52 layers) help where dense 3B collapsed?
# Key hypothesis: Depth enables reasoning even with limited active params

experiment_name: nemotron-depth
run_group: capacity-floor
parallelism: 1  # sequential to avoid rate limits
repeats: 1

models:
  # Nemotron-3-Nano: 31.6B total, 3.2B active, 52 layers, MoE + Mamba hybrid
  # The deepest MoE in this active-param range
  - name: openrouter/nvidia/nemotron-3-nano-30b-a3b
    repeats: 1
    idle_options:
      target_output_tokens: 6000
      disable_tools: true
      plugins:
        - module: memory_injection
          params:
            injection_type: identity
            interval: 5
            start_after: 3

idle_options:
  target_output_tokens: 6000
  shift_hours: 4.0
  max_iterations: 50
  disable_tools: true
  enable_web: false
  enable_render_svg: false
  enable_time_travel: false
  enable_broken_time_travel: false
  carry_forward_last_answer: false
  log_dir: logs
  artifact_dir: artifacts
  reasoning_summary: auto
  plugin_dir: plugins

render_options:
  html_dir: html
  collapse_backend: tfidf
  collapse_m_pct: 0.30
  collapse_threshold_pct: 0.15
  mlflow_dir: mlruns

open_html: false
run_name_template: "{experiment}-{model}-{ts}"
